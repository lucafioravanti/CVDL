{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GfNEZ57Uz-6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify work folder\n",
        "DATA_DIR = r'P:\\XAIM Biolab\\2nd sem CVDL/HAM10000/'   #SPECIFY YOURS\n",
        "\n",
        "# Load metadata\n",
        "num_rows_to_read = None  # To load the whole dataset     #Alternatively, make a titration to test it first by specifiying a number (for e.g \"5\" instead of None)\n",
        "data = pd.read_csv(os.path.join(DATA_DIR, 'HAM10000_metadata.csv'), nrows=num_rows_to_read)\n",
        "data['image_path'] = DATA_DIR + 'HAM10000_all_images/' + data['image_id'] + '.jpg'\n",
        "\n",
        "# Mapping dictionary\n",
        "lesion_type_dict = {\n",
        "    'nv': 'Melanocytic nevi',\n",
        "    'mel': 'Melanoma',\n",
        "    'bkl': 'Benign keratosis-like lesions ',\n",
        "    'bcc': 'Basal cell carcinoma',\n",
        "    'akiec': 'Actinic keratoses',\n",
        "    'vasc': 'Vascular lesions',\n",
        "    'df': 'Dermatofibroma'\n",
        "}\n",
        "\n",
        "# Add two more columns\n",
        "data['cell_type'] = data['dx'].map(lesion_type_dict.get)\n",
        "data['cell_type_idx'] = pd.Categorical(data['cell_type']).codes\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Swin Transformer model architecture\n",
        "class SwinTransformer(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SwinTransformer, self).__init__()\n",
        "        # Define the layers of the model\n",
        "        self.conv = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.linear = nn.Linear(64 * 224 * 224, num_classes)  # Assuming input image size is 224x224\n",
        "        self.dropout = nn.Dropout(0.5)  # Dropout layer with a dropout probability of 0.5\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the feature map\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "# Define the training dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.data.iloc[idx]['image_path']\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        label = torch.tensor(self.data.iloc[idx]['cell_type_idx'], dtype=torch.long)  # Convert label to torch.long\n",
        "        return img, label\n",
        "\n",
        "# Define transformations for data augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),  # Random horizontal flip for data augmentation\n",
        "    transforms.RandomRotation(15),  # Random rotation of up to 15 degrees for data augmentation\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create datasets and data loaders\n",
        "train_dataset = CustomDataset(train_data, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "test_dataset = CustomDataset(test_data, transform=transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "]))\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Instantiate the Swin Transformer model\n",
        "model = SwinTransformer(num_classes=7)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop with validation\n",
        "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=20):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "        correct_train_predictions = 0\n",
        "        total_train_predictions = 0\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_train_loss += loss.item() * images.size(0)\n",
        "\n",
        "            _, predicted_train = torch.max(outputs, 1)\n",
        "            correct_train_predictions += (predicted_train == labels).sum().item()\n",
        "            total_train_predictions += labels.size(0)\n",
        "\n",
        "        train_loss = running_train_loss / len(train_loader.dataset)\n",
        "        train_accuracy = correct_train_predictions / total_train_predictions\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        correct_val_predictions = 0\n",
        "        total_val_predictions = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_val_loss += loss.item() * images.size(0)\n",
        "\n",
        "                _, predicted_val = torch.max(outputs, 1)\n",
        "                correct_val_predictions += (predicted_val == labels).sum().item()\n",
        "                total_val_predictions += labels.size(0)\n",
        "\n",
        "        val_loss = running_val_loss / len(val_loader.dataset)\n",
        "        val_accuracy = correct_val_predictions / total_val_predictions\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Plot training and validation loss\n",
        "    plt.plot(train_losses, label='Training Loss', color='red')\n",
        "    plt.plot(val_losses, label='Validation Loss', color='green')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training and validation accuracy\n",
        "    plt.plot(train_accuracies, label='Training Accuracy', color='purple')\n",
        "    plt.plot(val_accuracies, label='Validation Accuracy', color='blue')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "cSU-nCZmU3Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with validation\n",
        "train_model(model, criterion, optimizer, train_loader, test_loader, num_epochs=20)\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "        total_predictions += labels.size(0)\n",
        "        all_labels.extend(labels.numpy())\n",
        "        all_predictions.extend(predicted.numpy())\n",
        "\n",
        "accuracy = correct_predictions / total_predictions\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "accuracy = correct_predictions / total_predictions\n"
      ],
      "metadata": {
        "id": "ojrg4iAuVARg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix and classification report\n",
        "confusion_mat = confusion_matrix(all_labels, all_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_predictions))\n",
        "\n",
        "# Visualize some predictions\n",
        "for i in range(min(5, len(all_labels))):  # Ensure that we don't exceed the length of the lists\n",
        "    print(f\"True Label: {all_labels[i]}, Predicted Label: {all_predictions[i]}\")\n"
      ],
      "metadata": {
        "id": "95N2rrWnVDmn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}